{{- $lighterServiceName := "" }}
{{ if and .Values.global ( .Values.global.isTopLevel) }}
{{ else }}
  {{- $lighterNamespace := index .Values "lighter" "namespace" -}}
  {{ range $index, $service := (lookup "v1" "Service" $lighterNamespace "").items }}
      {{ range $element := $service.spec.ports -}}
          {{ range $key, $value := $element -}}
              {{ if and (eq "port" $key) (eq 8080 $value) }}
                  {{- $lighterServiceName = $service.metadata.name }}
                  {{ break }}
              {{ end }}
          {{ end }}
      {{ end }}
  {{ end }}
  {{- if eq $lighterServiceName "" }}
  {{- fail ( printf "Cannot find lighter service in namespace: %s, update variable lighter.namespace in values.yaml" $lighterNamespace) }}
  {{- end }}
{{ end }}

apiVersion: v1
kind: Secret
metadata:
  name: {{ include "jupyterlab.fullname" . }}-secret
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "jupyterlab.labels" $ | trim | nindent 4 }}
type: Opaque
stringData:
  quickstart: |
    {
     "cells": [
      {
       "cell_type": "code",
       "execution_count": null,
       "id": "c72ef05c-13ee-4199-a002-b2be815ce419",
       "metadata": {},
       "outputs": [],
       "source": [
        "%%configure\n",
        "{\n",
        "    \"conf\":{\n",
        "    \"spark.kubernetes.container.image.pullPolicy\": \"Always\",\n",
        "    \"spark.driver.memory\" : \"500m\",\n",
        "    \"spark.executor.memory\": \"500m\"\n",
        "    }\n",
        "}"
       ]
      },
      {
       "cell_type": "code",
       "execution_count": null,
       "id": "94d68783-5c86-440e-84da-aa8be288cdd5",
       "metadata": {},
       "outputs": [],
       "source": [
        "df = spark.createDataFrame(\n",
        "    [\n",
        "        (1, \"foo\"),\n",
        "        (2, \"bar\"),\n",
        "    ],\n",
        "    [\"id\", \"label\"]\n",
        ")\n",
        "df.collect()"
       ]
      }
     ],
     "metadata": {
      "kernelspec": {
       "display_name": "PySpark",
       "language": "python",
       "name": "pysparkkernel"
      },
      "language_info": {
       "codemirror_mode": {
        "name": "python",
        "version": 3
       },
       "file_extension": ".py",
       "mimetype": "text/x-python",
       "name": "pyspark",
       "pygments_lexer": "python3"
      }
     },
     "nbformat": 4,
     "nbformat_minor": 5
    }

  tpcds: |
      {
      "cells": [
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "df378969-32dd-4d27-bbeb-89d532f5fb8b",
        "metadata": {},
        "outputs": [],
        "source": [
          "%%configure\n",
          "{\n",
          "    \"conf\":{\n",
          "    \"spark.kubernetes.executor.podNamePrefix\": \"tpcds-worker\",\n",
          "    \"spark.kubernetes.container.image.pullPolicy\": \"IfNotPresent\",\n",
          "    \"spark.driver.memory\" : \"2G\",\n",
          "    \"spark.executor.memory\": \"2G\"\n",
          "    }\n",
          "}"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "dbce441f-1825-4b07-9b1a-0a2c74bac6b8",
        "metadata": {},
        "outputs": [],
        "source": [
            "import subprocess, io\n",
            "command=\"pip install tpcds_pyspark sparkmeasure plotly kaleido pandas --user\"\n",
            "proc = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
            "for line in io.TextIOWrapper(proc.stdout, encoding=\"utf-8\"):\n",
            "    print(line.rstrip())"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "a483c3fc-7839-4e05-aa6e-a493265b2276",
        "metadata": {},
        "outputs": [],
        "source": [
            "# Download TPC-DS data\n",
            "import subprocess, io\n",
            "command=\"[ ! -d '/opt/spark/work-dir/tpcds_10' ] && wget -O /opt/spark/work-dir/tpcds_10.zip https://sparkdltrigger.web.cern.ch/sparkdltrigger/TPCDS/tpcds_10.zip && unzip /opt/spark/work-dir/tpcds_10.zip\"\n",
            "proc = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n",
            "for line in io.TextIOWrapper(proc.stdout, encoding=\"utf-8\"):\n",
            "    print(line.rstrip())"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "923963fd-5166-4232-bc43-b9ca35df5080",
        "metadata": {},
        "outputs": [],
        "source": [
            "from tpcds_pyspark import TPCDS\n",
            "\n",
            "# Basic configuration, use tpcds 10G scale and just run 2 queries to get started\n",
            "tpcds = TPCDS(data_path=\"/opt/spark/work-dir/tpcds_10\", queries=['q1', 'q2'])\n"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "25272237-dd08-4321-8ef8-f6d3a6f4d3d3",
        "metadata": {},
        "outputs": [],
        "source": [
            "# Map TPC-DS tables into temporary views\n",
            "tpcds.map_tables() "
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "a0bbfd33-fbef-4495-9f8d-1a9d44a33039",
        "metadata": {},
        "outputs": [],
        "source": [
            "# Run the TPC-DS workload\n",
            "results = tpcds.run_TPCDS()"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "81f636cb-bad0-4007-98ad-576213f2f5cb",
        "metadata": {},
        "outputs": [],
        "source": [
            "# Print the test output and reports\n",
            "tpcds.print_test_results()"
        ]
        },
        {
        "cell_type": "markdown",
        "execution_count": null,
        "id": "b5ce4efc-a081-4e69-b989-e6bcb10e80b1",
        "metadata": {},
        "outputs": [],
        "source": [
            "## Test data analysis\n"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "05f28ad5-c41d-4420-a78d-b76032efc0e2",
        "metadata": {},
        "outputs": [],
        "source": [
            "tpcds.aggregated_results_pdf"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "6cadc03c-00a2-4e28-8605-425b6dbc027f",
        "metadata": {},
        "outputs": [],
        "source": [
            "tpcds.grouped_results_pdf"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "11c42044-3479-49f3-89d1-581ce27a6e5d",
        "metadata": {},
        "outputs": [],
        "source": [
            "tpcds.grouped_results_pdf.columns"
        ]
        },
        {
        "cell_type": "markdown",
        "execution_count": null,
        "id": "cb27bd04-1e7b-4d2e-8926-3e06e10a21c4",
        "metadata": {},
        "outputs": [],
        "source": [
            "## Example of data plotting"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "e088c410-2903-4aab-808c-90506cf45059",
        "metadata": {},
        "outputs": [],
        "source": [
            "import plotly.express as px"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "1fff0ded-fb22-401d-a445-5b0248001bbc",
        "metadata": {},
        "outputs": [],
        "source": [
            "%%spark -o image_out\n",
            "import io, base64, sys\n",
            "from pyspark.sql.types import StructType,StructField, StringType\n",
            "\n",
            "iobytes2 = io.BytesIO()\n",
            "fig = px.bar(tpcds.grouped_results_pdf.reset_index(), x='query', y=['executorRunTime', 'executorCpuTime'], \n",
            "             title='Executor Run Time and CPU Time per Query', barmode='group',\n",
            "             labels={'value': 'Time (ms)', 'variable': 'Metric'}) # Customizing y-axis label and legend title)\n",
            "\n",
            "fig_image_bytes = fig.to_image(format=\"jpg\")\n",
            "\n",
            "# Save the image to an io.BytesIO object\n",
            "iobytes2 = io.BytesIO(fig_image_bytes)\n",
            "\n",
            "\n",
            "iobytes2.seek(0)\n",
            "my_base64_jpgData2 = base64.b64encode(iobytes2.read()).decode()\n",
            "schema = StructType([\n",
            "    StructField(\"content\", StringType(), nullable=False)\n",
            "])\n",
            "image_out = spark.createDataFrame([(my_base64_jpgData2,)], schema)\n",
            "\n"
        ]
        },
        {
        "cell_type": "code",
        "execution_count": null,
        "id": "799afa24-1ed5-4aa4-86bb-f92e340c0c4b",
        "metadata": {},
        "outputs": [],
        "source": [
            "%%local\n",
            "import base64\n",
            "from IPython import display\n",
            "display.Image(base64.b64decode(image_out['content'][0]))\n"
        ]
        }
      ],
      "metadata": {
        "kernelspec": {
        "display_name": "PySpark",
        "language": "python",
        "name": "pysparkkernel"
        },
        "language_info": {
        "codemirror_mode": {
          "name": "python",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "pyspark",
        "pygments_lexer": "python3"
        }
      },
      "nbformat": 4,
      "nbformat_minor": 5
      }

  jupyter_notebook_config: |
    c = get_config()
    c.NotebookApp.password = '{{ .Values.jupyterPassword}}'